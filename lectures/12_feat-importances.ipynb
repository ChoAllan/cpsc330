{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](img/330-banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Lecture 12: Feature importances\n",
    "\n",
    "UBC 2020-21\n",
    "\n",
    "Instructor: Varada Kolhatkar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import string\n",
    "import sys\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"code/.\")\n",
    "\n",
    "from plotting_functions import *\n",
    "from sklearn import datasets\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from utils import *\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Learning outcomes \n",
    "\n",
    "From this lecture, students are expected to be able to:\n",
    "\n",
    "- Interpret the coefficients of linear regression for ordinal, one-hot encoded categorical, and scaled numeric features. \n",
    "- Explain why interpretability is important in ML.\n",
    "- Use `feature_importances_` attribute of `sklearn` models and interpret its output. \n",
    "- Use `eli5` to get feature importances of non `sklearn` mode\n",
    "\n",
    "ls and interpret its output. \n",
    "- Apply SHAP to assess feature importances and interpret model predictions. \n",
    "- Explain force plot, summary plot, and dependence plot produced with shapely values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lecture plan for today\n",
    "\n",
    "- Announcements (~2 mins)\n",
    "- Live explanation: Feature importances in linear models (~30 mins)\n",
    "- Break (~5 mins)\n",
    "- Summary of the pre-watched video (~2 mins)\n",
    "- SHAP (~30 mins)\n",
    "- Summary and wrap up (~2 mins)\n",
    "- Time for midterm-related questions  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Announcement\n",
    "\n",
    "- Midterm on Thursday. \n",
    "    - Bring your laptop. Make sure that it's fully charged. \n",
    "    - Bring your UBC ID Card. \n",
    "- Homework 5 extension\n",
    "    - New due date is Oct 27th at 11:59pm    \n",
    "- Three OH are scheduled earlier in the week. \n",
    "- My OH on Thursday has been cancelled.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data \n",
    "\n",
    "In this lecture, we'll be using [Kaggle House Prices dataset](https://www.kaggle.com/c/home-data-for-ml-course/), the dataset we used in lecture 2. As usual, to run this notebook you'll need to download the data. Unzip the data into a subdirectory called `data`. For this dataset, train and test have already been separated. We'll be working with the train portion in this lecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/housing-kaggle/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14344/1451838093.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/housing-kaggle/train.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m123\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/housing-kaggle/train.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/housing-kaggle/train.csv\")\n",
    "train_df, test_df = train_test_split(df, test_size = 0.10, random_state=123)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- The prediction task is predicting `SalePrice` given features related to properties.  \n",
    "- Note that the target is numeric, not categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Let's separate `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X_train = train_df.drop(columns=['SalePrice'])\n",
    "y_train = train_df['SalePrice']\n",
    "\n",
    "X_test = test_df.drop(columns=['SalePrice'])\n",
    "y_test = test_df['SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's identify feature types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "drop_features = [\"Id\"]\n",
    "numeric_features = [\n",
    "    \"BedroomAbvGr\",\n",
    "    \"KitchenAbvGr\", \n",
    "    \"LotFrontage\",\n",
    "    \"LotArea\",\n",
    "    \"OverallQual\",\n",
    "    \"OverallCond\",\n",
    "    \"YearBuilt\",\n",
    "    \"YearRemodAdd\",\n",
    "    \"MasVnrArea\",\n",
    "    \"BsmtFinSF1\",\n",
    "    \"BsmtFinSF2\",\n",
    "    \"BsmtUnfSF\",\n",
    "    \"TotalBsmtSF\",\n",
    "    \"1stFlrSF\",\n",
    "    \"2ndFlrSF\",\n",
    "    \"LowQualFinSF\",\n",
    "    \"GrLivArea\",\n",
    "    \"BsmtFullBath\",\n",
    "    \"BsmtHalfBath\",\n",
    "    \"FullBath\",\n",
    "    \"HalfBath\",\n",
    "    \"TotRmsAbvGrd\",\n",
    "    \"Fireplaces\",\n",
    "    \"GarageYrBlt\",\n",
    "    \"GarageCars\",\n",
    "    \"GarageArea\",\n",
    "    \"WoodDeckSF\",\n",
    "    \"OpenPorchSF\",\n",
    "    \"EnclosedPorch\",\n",
    "    \"3SsnPorch\",\n",
    "    \"ScreenPorch\",\n",
    "    \"PoolArea\",\n",
    "    \"MiscVal\",\n",
    "    \"YrSold\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ordinal_features_reg = [\n",
    "    \"ExterQual\",\n",
    "    \"ExterCond\",\n",
    "    \"BsmtQual\",\n",
    "    \"BsmtCond\",\n",
    "    \"HeatingQC\",\n",
    "    \"KitchenQual\",\n",
    "    \"FireplaceQu\",\n",
    "    \"GarageQual\",\n",
    "    \"GarageCond\",\n",
    "    \"PoolQC\",\n",
    "]\n",
    "ordering = [\n",
    "    \"Po\",\n",
    "    \"Fa\",\n",
    "    \"TA\",\n",
    "    \"Gd\",\n",
    "    \"Ex\",\n",
    "]  # if N/A it will just impute something, per below\n",
    "ordering_ordinal_reg = [ordering] * len(ordinal_features_reg)\n",
    "ordering_ordinal_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ordinal_features_oth = [\n",
    "    \"BsmtExposure\",\n",
    "    \"BsmtFinType1\",\n",
    "    \"BsmtFinType2\",\n",
    "    \"Functional\",\n",
    "    \"Fence\",\n",
    "]\n",
    "ordering_ordinal_oth = [\n",
    "    ['NA', 'No', 'Mn', 'Av', 'Gd'],\n",
    "    ['NA', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'],\n",
    "    ['NA', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'],\n",
    "    ['Sal','Sev','Maj2','Maj1','Mod','Min2','Min1','Typ'],    \n",
    "    ['NA', 'MnWw', 'GdWo', 'MnPrv', 'GdPrv']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "categorical_features = list(\n",
    "    set(X_train.columns)\n",
    "    - set(numeric_features)\n",
    "    - set(ordinal_features_reg)\n",
    "    - set(ordinal_features_oth)    \n",
    "    - set(drop_features)\n",
    ")\n",
    "categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "\n",
    "numeric_transformer = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n",
    "ordinal_transformer_reg = make_pipeline(\n",
    "    SimpleImputer(strategy=\"most_frequent\"),\n",
    "    OrdinalEncoder(categories=ordering_ordinal_reg),\n",
    ")\n",
    "\n",
    "ordinal_transformer_oth = make_pipeline(\n",
    "    SimpleImputer(strategy=\"most_frequent\"),\n",
    "    OrdinalEncoder(categories=ordering_ordinal_oth),\n",
    ")\n",
    "\n",
    "categorical_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "    OneHotEncoder(handle_unknown=\"ignore\", sparse=False),\n",
    ")\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (\"drop\", drop_features),\n",
    "    (numeric_transformer, numeric_features),\n",
    "    (ordinal_transformer_reg, ordinal_features_reg),\n",
    "    (ordinal_transformer_oth, ordinal_features_oth),    \n",
    "    (categorical_transformer, categorical_features),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "preprocessor.fit(X_train)\n",
    "preprocessor.named_transformers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ohe_columns = list(\n",
    "    preprocessor.named_transformers_[\"pipeline-4\"]\n",
    "    .named_steps[\"onehotencoder\"]\n",
    "    .get_feature_names(categorical_features)\n",
    ")\n",
    "new_columns = numeric_features + ordinal_features_reg + ordinal_features_oth + ohe_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train_enc = pd.DataFrame(preprocessor.transform(X_train), index=X_train.index, columns=new_columns)\n",
    "X_train_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's look at the correlations between various features with other features and the target in our encoded data (first row/column). \n",
    "- In simple terms here is how you can interpret correlations between two variables $X$ and $Y$:\n",
    "  - If $Y$ goes up when $X$ goes up, we say $X$ and $Y$ are positively correlated.\n",
    "  - If $Y$ goes down when $X$ goes up, we say $X$ and $Y$ are negatively correlated.\n",
    "  - If $Y$ is unchanged when $X$ changes, we say $X$ and $Y$ are uncorrelated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "cor = pd.concat((y_train, X_train_enc),axis=1).iloc[:,:15].corr();\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.set(font_scale=1)\n",
    "sns.heatmap(cor, annot=True, cmap=plt.cm.Blues);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We can immediately see that `SalePrice` is highly correlated with `OverallQual`.\n",
    "- This is an early hint that `OverallQual` is a useful feature in predicting `SalePrice`.\n",
    "- However, this approach is **extremely simplistic**.\n",
    "  - It only looks at each feature in isolation.\n",
    "  - It only looks at linear associations:\n",
    "    - What if `SalePrice` is high when `BsmtFullBath` is 2 or 3, but low when it's 0, 1, or 4? They might seem uncorrelated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "cor = pd.concat((y_train, X_train_enc),axis=1).iloc[:,10:15].corr();\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.set(font_scale=1)\n",
    "sns.heatmap(cor, annot=True, cmap=plt.cm.Blues);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Looking at this diagram also tells us the relationship between features. \n",
    "  - For example, `1stFlrSF` and `TotalBsmtSF` are highly correlated. \n",
    "  - Do we need both of them?\n",
    "  - If our model says `1stFlrSF` is very important and `TotalBsmtSF` is very unimportant, do we trust those values?\n",
    "  - Maybe `TotalBsmtSF` only \"becomes important\" if `1stFlrSF` is removed.\n",
    "  - Sometimes the opposite happens: a feature only becomes important if another feature is _added_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature importances in linear models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Like logistic regression, with linear regression we can look at the _coefficients_ for each feature.\n",
    "- Overall idea: predicted price = intercept + $\\sum_i$ coefficient i $\\times$ feature i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = make_pipeline(preprocessor, Ridge())\n",
    "lr.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at the coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_coefs = pd.DataFrame(data=lr[1].coef_, index=new_columns, columns=[\"Coefficient\"])\n",
    "lr_coefs.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interpreting coefficients of different types of features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ordinal features\n",
    "\n",
    "- The ordinal features are easiest to interpret. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ordinal_features_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_coefs.loc[\"ExterQual\", 'Coefficient']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Increasing by one category of exterior quality (e.g. good -> excellent) increases the predicted price by $\\sim\\$4195$.\n",
    "  - Wow, that's a lot! \n",
    "  - Remember this is just what the model has learned. It doesn't tell us how the world works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "one_example = X_test[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_example['ExterQual']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's perturb the example and change `ExterQual` to `Ex`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_example_perturbed = one_example.copy()\n",
    "one_example_perturbed[\"ExterQual\"] = 'Ex' # Change Gd to Ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_example_perturbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "one_example_perturbed['ExterQual']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How does the prediction change after changing `ExterQual` from `Gd` to `Ex`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print('Prediction on the original example: ', lr.predict(one_example))\n",
    "print('Prediction on the perturbed example: ', lr.predict(one_example_perturbed))\n",
    "print(\"After changing ExterQual from Gd to Ex increased the prediction by: \", lr.predict(one_example_perturbed) - lr.predict(one_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's exactly the learned coefficient for `ExterQual`! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_coefs.loc[\"ExterQual\", 'Coefficient']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our interpretation is correct! \n",
    "- Increasing by one category of exterior quality (e.g. good -> excellent) increases the predicted price by $\\sim\\$4195$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- What about the categorical features?\n",
    "- We have created a number of columns for each category with OHE and each category gets it's own coefficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "lr_coefs_landslope = lr_coefs[lr_coefs.index.str.startswith(\"LandSlope\")]\n",
    "lr_coefs_landslope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We can talk about switching from one of these categories to another by picking a \"reference\" category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "lr_coefs_landslope - lr_coefs_landslope.loc[\"LandSlope_Gtl\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you change the category from `LandSlope_Gtl` to `LandSlope_Mod` the prediction price goes up by $\\sim\\$6963$\n",
    "- If you change the category from `LandSlope_Gtl` to `LandSlope_Sev` the prediction price goes down by $\\sim\\$8334$\n",
    "\n",
    "\n",
    "Note that this might not make sense in the real world but this is what our model decided to learn given this small amount of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_coefs.sort_values(by=\"Coefficient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- For example, the above coefficient says that \"If the roof is made of clay or tile, the predicted price is \\\\$191K less\"?\n",
    "- Do we believe these interpretations??\n",
    "  - Do we believe this is how the predictions are being computed? Yes.\n",
    "  - Do we believe that this is how the world works? No. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "```{note}\n",
    "If you did `drop='first'` (we didn't) then you already have a reference class, and all the values are with respect to that one. The interpretation depends on whether we did `drop='first'`, hence the hassle.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interpreting coefficients of numeric features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's look at coefficients of `PoolArea` and `LotFrontage`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "lr_coefs.loc[[\"PoolArea\", \"LotArea\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuition: \n",
    "\n",
    "- Tricky because numeric features are **scaled**! \n",
    "- **Increasing** `PoolArea` by 1 scaled unit **increases** the predicted price by $\\sim\\$2822$.\n",
    "- **Increasing** `LotFrontage` by 1 scaled unit **decreases** the predicted price by $\\sim\\$1578$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Does that sound reasonable?\n",
    "\n",
    "- For `PoolArea`, yes. \n",
    "- For `LotFrontage`, that's surprising. Something positive would have made more sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "It's not the case here but maybe the problem is that `LotFrontage` and `LotArea` are very correlated. `LotArea` has a larger positive coefficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "cor = X_train_enc[numeric_features[:5]].corr()\n",
    "sns.heatmap(cor, annot=True, cmap=plt.cm.Blues);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "BTW, let's make sure the predictions behave as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_example = X_test[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's perturb the example and add 1 to the `LotArea`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_example_perturbed = one_example.copy()\n",
    "one_example_perturbed[\"LotArea\"] += 1 # add 1 to the LotArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_example_perturbed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Prediction on the original example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.predict(one_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction on the perturbed example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "lr.predict(one_example_perturbed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- What's the difference between prediction? \n",
    "- Does the difference make sense given the coefficient of the feature? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "lr.predict(one_example_perturbed) - lr.predict(one_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "lr_coefs.loc[[\"LotArea\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Why did the prediction only go up by \\\\$0.57 instead of \\\\$5109? \n",
    "- This is an issue of units. `LotArea` is in sqft, but the coefficient is not $\\$5109/\\text{sqft}$ **because we scaled the features**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example showing how can we interpret coefficients of scaled features. \n",
    "\n",
    "- The scaler subtracted the mean and divided by the standard deviation.\n",
    "- The division actually changed the scale! \n",
    "- For the unit conversion, we don't care about the subtraction, but only the scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "scaler = preprocessor.named_transformers_['pipeline-1']['standardscaler']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "np.sqrt(scaler.var_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "lr_scales = pd.DataFrame(data=np.sqrt(scaler.var_), index=numeric_features, columns=[\"Scale\"])\n",
    "lr_scales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It seems like `LotArea` was divided by 8994.471032 sqft. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "lr_coefs.loc[\"LotArea\",\"Coefficient\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "lr_coefs.loc[\"LotArea\",\"Coefficient\"]/lr_scales.loc[\"LotArea\",\"Scale\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "lr_coefs.loc[[\"LotArea\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- The coefficient tells us that if we increase the **scaled** `LotArea` by one unit the price would go up by $\\approx\\$5109$. \n",
    "- One scaled unit represents $\\sim 8994$ sq feet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- So if I increase original `LotArea` by one square foot then the predicted price would go up by this amount: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "5109.356718094072/8994.471032 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- This makes much more sense. Now we get the number we got before. \n",
    "- That said don't read too much into these coefficients without statistical training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interim summary\n",
    "\n",
    "- Correlation among features might make coefficients completely uninterpretable. \n",
    "- Fairly straightforward to interpret coefficients of ordinal features. \n",
    "- In categorical features, it's often helpful to consider one category as a reference point and think about relative importance. \n",
    "- For numeric features, relative importance is meaningful after scaling.\n",
    "- You have to be careful about the scale of the feature when interpreting the coefficients. \n",
    "- Remember that explaining the model $\\neq$ explaining the data.  \n",
    "- the coefficients tell us only about the model and they might not accurately reflect the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break (5 min)\n",
    "\n",
    "![](img/eva-coffee.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpretability of ML models: Motivations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why model interpretability? \n",
    "\n",
    "- Ability to interpret ML models is crucial in many applications such as banking, healthcare, and criminal justice.\n",
    "- It can be leveraged by domain experts to diagnose systematic errors and underlying biases of complex ML systems. \n",
    "\n",
    "<center>\n",
    "<img src=\"img/shap_example.png\" width=\"600\" height=\"600\">\n",
    "</center>\n",
    "    \n",
    "[Source](https://github.com/slundberg/shap/blob/master/docs/presentations/February%202018%20Talk.pptx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is model interpretability? \n",
    "\n",
    "- In this course, our definition of model iterpretability will be looking at **feature importances**.\n",
    "- There is more to interpretability than feature importances, but it's a good start!\n",
    "- Resource: \n",
    "    - [Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/interpretability-importance.html)\n",
    "    - [Yann LeCun, Kilian Weinberger, Patrice Simard, and Rich Caruana: Panel debate on interpretability](https://vimeo.com/252187813)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data\n",
    "\n",
    "- Let's work with [the adult census data set](https://www.kaggle.com/uciml/adult-census-income) from last lecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "adult_df_large = pd.read_csv(\"data/adult.csv\")\n",
    "train_df, test_df = train_test_split(adult_df_large, test_size=0.2, random_state=42)\n",
    "train_df_nan = train_df.replace(\"?\", np.NaN)\n",
    "test_df_nan = test_df.replace(\"?\", np.NaN)\n",
    "train_df_nan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "numeric_features = [\"age\", \"fnlwgt\", \"capital.gain\", \"capital.loss\", \"hours.per.week\"]\n",
    "categorical_features = [\n",
    "    \"workclass\",\n",
    "    \"marital.status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"native.country\",\n",
    "]\n",
    "ordinal_features = [\"education\"]\n",
    "binary_features = [\"sex\"]\n",
    "drop_features = [\"race\", \"education.num\"]\n",
    "target_column = \"income\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "education_levels = [\n",
    "    \"Preschool\",\n",
    "    \"1st-4th\",\n",
    "    \"5th-6th\",\n",
    "    \"7th-8th\",\n",
    "    \"9th\",\n",
    "    \"10th\",\n",
    "    \"11th\",\n",
    "    \"12th\",\n",
    "    \"HS-grad\",\n",
    "    \"Prof-school\",\n",
    "    \"Assoc-voc\",\n",
    "    \"Assoc-acdm\",\n",
    "    \"Some-college\",\n",
    "    \"Bachelors\",\n",
    "    \"Masters\",\n",
    "    \"Doctorate\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(education_levels) == set(train_df[\"education\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "numeric_transformer = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n",
    "tree_numeric_transformer = make_pipeline(SimpleImputer(strategy=\"median\"))\n",
    "\n",
    "categorical_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "    OneHotEncoder(handle_unknown=\"ignore\"),\n",
    ")\n",
    "\n",
    "ordinal_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "    OrdinalEncoder(categories=[education_levels], dtype=int),\n",
    ")\n",
    "\n",
    "binary_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "    OneHotEncoder(drop=\"if_binary\", dtype=int),\n",
    ")\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (\"drop\", drop_features),\n",
    "    (numeric_transformer, numeric_features),\n",
    "    (ordinal_transformer, ordinal_features),\n",
    "    (binary_transformer, binary_features),\n",
    "    (categorical_transformer, categorical_features),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train = train_df_nan.drop(columns=[target_column])\n",
    "y_train = train_df_nan[target_column]\n",
    "\n",
    "X_test = test_df_nan.drop(columns=[target_column])\n",
    "y_test = test_df_nan[target_column]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Do we have class imbalance? \n",
    "\n",
    "- There is class imbalance. But without any context, both classes seem equally important. \n",
    "- Let's use accuracy as our metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_df_nan[\"income\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "scoring_metric = \"accuracy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's store all the results in a dictionary called `results`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "scoring_metric = \"accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "pipe_lr = make_pipeline(\n",
    "    preprocessor, LogisticRegression(max_iter=2000, random_state=123)\n",
    ")\n",
    "pipe_rf = make_pipeline(preprocessor, RandomForestClassifier(random_state=123))\n",
    "pipe_xgb = make_pipeline(preprocessor, XGBClassifier(random_state=123, eval_metric=\"logloss\", verbosity=0))\n",
    "pipe_lgbm = make_pipeline(preprocessor, LGBMClassifier(random_state=123))\n",
    "classifiers = {\n",
    "    \"logistic regression\": pipe_lr,\n",
    "    \"random forest\": pipe_rf,\n",
    "    \"XGBoost\": pipe_xgb,\n",
    "    \"LightGBM\": pipe_lgbm,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dummy = DummyClassifier(strategy=\"stratified\")\n",
    "results[\"Dummy\"] = mean_std_cross_val_scores(\n",
    "    dummy, X_train, y_train, return_train_score=True, scoring=scoring_metric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for (name, model) in classifiers.items():\n",
    "    results[name] = mean_std_cross_val_scores(\n",
    "        model, X_train, y_train, return_train_score=True, scoring=scoring_metric\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- One problem is that often simple models are interpretable but not accurate.\n",
    "- But more complex models (e.g., LightGBM) are less interpretable.\n",
    "\n",
    "<center>\n",
    "<img src=\"img/shap_motivation.png\" width=\"600\" height=\"600\">\n",
    "</center>    \n",
    "\n",
    "[Source](https://github.com/slundberg/shap/blob/master/docs/presentations/February%202018%20Talk.pptx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature importances in linear models\n",
    "\n",
    "- Simpler models are often more interpretable but less accurate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create and fit a pipeline with preprocessor and logistic regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr = make_pipeline(preprocessor, LogisticRegression(max_iter=2000, random_state=2))\n",
    "pipe_lr.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ohe_feature_names = (\n",
    "    pipe_rf.named_steps[\"columntransformer\"]\n",
    "    .named_transformers_[\"pipeline-4\"]\n",
    "    .named_steps[\"onehotencoder\"]\n",
    "    .get_feature_names()\n",
    "    .tolist()\n",
    ")\n",
    "feature_names = (\n",
    "    numeric_features + ordinal_features + binary_features + ohe_feature_names\n",
    ")\n",
    "feature_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"coefficient\": pipe_lr.named_steps[\"logisticregression\"].coef_[0].tolist(),\n",
    "    \"magnitude\": np.absolute(\n",
    "        pipe_lr.named_steps[\"logisticregression\"].coef_[0].tolist()\n",
    "    ),\n",
    "}\n",
    "coef_df = pd.DataFrame(data, index=feature_names).sort_values(\n",
    "    \"magnitude\", ascending=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "coef_df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Increasing `capital.gain` is likely to push the prediction towards \">50k\" income class \n",
    "- Whereas occupation of private house service is likely to push the prediction towards \"<=50K\" income. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Can we get feature importances for non-linear models? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model interpretability beyond linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will be looking at three ways for model interpretability. \n",
    "\n",
    "- `sklearn` `feature_importances_`    \n",
    "- [eli5](https://eli5.readthedocs.io/en/latest/tutorials/black-box-text-classifiers.html#lime-tutorial) (stands for \"explain like I'm 5\") \n",
    "- [SHAP](https://github.com/slundberg/shap)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `sklearn` `feature_importances_`\n",
    "\n",
    "- Many `sklearn` models have `feature_importances_` attribute.\n",
    "- For tree-based models it's calculated based on impurity (gini index or information gain).\n",
    "- For example, let's look at `feature_importances_` of `RandomForestClassifier`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's create and fit a pipeline with preprocessor and random forest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pipe_rf = make_pipeline(preprocessor, RandomForestClassifier(random_state=2))\n",
    "pipe_rf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Which features are driving the predictions the most? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"Importance\": pipe_rf.named_steps[\"randomforestclassifier\"].feature_importances_,\n",
    "}\n",
    "imps = pd.DataFrame(data=data, index=feature_names,).sort_values(\n",
    "    by=\"Importance\", ascending=False\n",
    ")[:10]\n",
    "imps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Key point \n",
    "\n",
    "- Unlike the linear model coefficients, `feature_importances_` **do not have a sign**!\n",
    "  - They tell us about importance, but not an \"up or down\".\n",
    "  - Indeed, increasing a feature may cause the prediction to first go up, and then go down.\n",
    "  - This cannot happen in linear models, because they are linear. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Do these importances match with importances identified by logistic regression? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"random forest importance\": pipe_rf.named_steps[\n",
    "        \"randomforestclassifier\"\n",
    "    ].feature_importances_,\n",
    "    \"logistic regression importances\": pipe_lr.named_steps[\"logisticregression\"]\n",
    "    .coef_[0]\n",
    "    .tolist(),\n",
    "}\n",
    "imps = pd.DataFrame(\n",
    "    data=data,\n",
    "    index=feature_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "imps.sort_values(by=\"random forest importance\", ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Both models agree on `age`, `education`, `capital.gain`\n",
    "- The actual numbers for random forests and logistic regression are not really comparable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How can we get feature importances for non `sklearn` models? \n",
    "\n",
    "- One way to do it is by using a tool called [`eli5`](https://eli5.readthedocs.io/en/latest/overview.html).\n",
    "\n",
    "You'll have to install it\n",
    "\n",
    "```\n",
    "conda install -c conda-forge eli5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at feature importances for `XGBClassifier`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "```{note}\n",
    "For some reason, `eli5.explain_weights` doesn't work with the latest version of `xgboost`. I downgraded the `xgboost` version using this command to make it work `conda install -c conda-forge py-xgboost==1.3.0`. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import eli5\n",
    "# pipe_xgb = make_pipeline(preprocessor, XGBClassifier(random_state=123, eval_metric=\"logloss\", verbosity=0))\n",
    "# pipe_xgb.fit(X_train, y_train);\n",
    "# eli5.explain_weights(pipe_xgb.named_steps[\"xgbclassifier\"], feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at feature importances for `LGBMClassifier`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lgbm = make_pipeline(preprocessor, LGBMClassifier(random_state=123))\n",
    "pipe_lgbm.fit(X_train, y_train)\n",
    "eli5.explain_weights(\n",
    "    pipe_lgbm.named_steps[\"lgbmclassifier\"], feature_names=feature_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can also look at feature importances for `RandomForestClassifier`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.explain_weights(\n",
    "    pipe_rf.named_steps[\"randomforestclassifier\"], feature_names=feature_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's compare them with weights what we got with `sklearn` `feature_importances_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"Importance\": pipe_rf.named_steps[\"randomforestclassifier\"].feature_importances_,\n",
    "}\n",
    "pd.DataFrame(data=data, index=feature_names,).sort_values(\n",
    "    by=\"Importance\", ascending=False\n",
    ")[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- These values tell us globally about which features are important.\n",
    "- But what if you want to explain a _specific_ prediction. \n",
    "- Some fancier tools can help us do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SHAP  (SHapley Additive exPlanations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### SHAP  (SHapley Additive exPlanations)\n",
    "- A sophisticated measure of the contribution of each feature.\n",
    "- [Lundberg and Lee, 2017](https://arxiv.org/pdf/1705.07874.pdf)\n",
    "- We won't go in details. You may refer to [Scott Lundberg's GitHub repo](https://github.com/slundberg/shap) if you are interested to know more. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### General idea\n",
    "\n",
    "<center>\n",
    "<img src=\"img/shap_example.png\" width=\"600\" height=\"600\">\n",
    "</center>\n",
    "\n",
    "[Source](https://github.com/slundberg/shap/blob/master/docs/presentations/February%202018%20Talk.pptx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### General idea\n",
    "\n",
    "- Provides following kind of explanation\n",
    "    - Start at a base rate (e.g., how often people get their loans rejected).\n",
    "    - Add one feature at a time and see how it impacts the decision. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"img/shap_explanation2.png\" width=\"1000\" height=\"1000\">\n",
    "</center>\n",
    "\n",
    "[Source](https://github.com/slundberg/shap/blob/master/docs/presentations/February%202018%20Talk.pptx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Let's try it out on tree-based models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "First you'll have to install it. \n",
    "\n",
    "```\n",
    "pip install shap\n",
    "or\n",
    "conda install -c conda-forge shap\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's create train and test dataframes with our transformed features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14344/1484340344.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m X_train_enc = pd.DataFrame(\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocessor' is not defined"
     ]
    }
   ],
   "source": [
    "X_train_enc = pd.DataFrame(\n",
    "    data=preprocessor.transform(X_train).toarray(),\n",
    "    columns=feature_names,\n",
    "    index=X_train.index,\n",
    ")\n",
    "X_train_enc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_test_enc = pd.DataFrame(\n",
    "    data=preprocessor.transform(X_test).toarray(),\n",
    "    columns=feature_names,\n",
    "    index=X_test.index,\n",
    ")\n",
    "\n",
    "X_test_enc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's get SHAP values for train and test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "lgbm_explainer = shap.TreeExplainer(pipe_lgbm.named_steps[\"lgbmclassifier\"])\n",
    "train_lgbm_shap_values = lgbm_explainer.shap_values(X_train_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipe_lgbm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14344/4162960892.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpipe_lgbm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pipe_lgbm' is not defined"
     ]
    }
   ],
   "source": [
    "pipe_lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lgbm_shap_values[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lgbm_shap_values = lgbm_explainer.shap_values(X_test_enc)\n",
    "test_lgbm_shap_values[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- For classification it's a bit confusing. It gives SHAP arrays both classes.\n",
    "- Let's stick to shap values for class 1, i.e., income > 50K. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each example and each feature we have a SHAP value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lgbm_shap_values[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at the average SHAP values associated with each feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = np.abs(train_lgbm_shap_values[1]).mean(0)\n",
    "pd.DataFrame(data=values, index=feature_names, columns=[\"SHAP\"]).sort_values(\n",
    "    by=\"SHAP\", ascending=False\n",
    ")[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of this as global feature importances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SHAP plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# load JS visualization code to notebook\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dependence plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"age\", train_lgbm_shap_values[1], X_train_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The plot above shows effect of `age` feature on the prediction. \n",
    "\n",
    "- Each dot is a single prediction for examples above.\n",
    "- The x-axis represents values of the feature age (scaled).\n",
    "- The y-axis is the SHAP value for that feature, which represents how much knowing that feature's value changes the output of the model for that example's prediction. \n",
    "- Lower values of age have smaller SHAP values for class \">50K\".\n",
    "- Similarly, higher values of age also have a bit smaller SHAP values for class \">50K\", which makes sense.  \n",
    "- There is some optimal value of age between scaled age of 1 which gives highest SHAP values for for class \">50K\". \n",
    "- Ignore the colour for now. The color corresponds to a second feature (education feature in this case) that may have an interaction effect with the feature we are plotting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(train_lgbm_shap_values[1], X_train_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The plot shows the most important features for predicting the class. It also shows the direction of how it's going to drive the prediction.  \n",
    "\n",
    "- Presence of the marital status of Married-civ-spouse seems to have bigger SHAP values for class 1 and absence seems to have smaller SHAP values for class 1. \n",
    "- Higher levels of education seem to have bigger SHAP values for class 1 whereas smaller levels of education have smaller SHAP values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Force plot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Let's try to explain predictions on a couple of examples from the test data. \n",
    "- I'm sampling some examples where target is <=50K and some examples where target is >50K. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "y_test_reset = y_test.reset_index(drop=True)\n",
    "y_test_reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "l50k_ind = y_test_reset[y_test_reset == \"<=50K\"].index.tolist()\n",
    "g50k_ind = y_test_reset[y_test_reset == \">50K\"].index.tolist()\n",
    "\n",
    "ex_l50k_index = l50k_ind[10]\n",
    "ex_g50k_index = g50k_ind[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example with prediction <=50K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lgbm.named_steps[\"lgbmclassifier\"].predict_proba(X_test_enc)[ex_l50k_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pipe_lgbm.named_steps[\"lgbmclassifier\"].predict(X_test_enc, raw_score=True)[ex_l50k_index] # raw score of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_explainer.expected_value[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "shap.force_plot(\n",
    "    lgbm_explainer.expected_value[1],\n",
    "    test_lgbm_shap_values[1][ex_l50k_index, :],\n",
    "    X_test_enc.iloc[ex_l50k_index, :],\n",
    "    matplotlib=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The raw model score is smaller than the base value so the prediction is <=50K (because we are consider the shap values for class 1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example with prediction >50K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lgbm.named_steps[\"lgbmclassifier\"].predict_proba(X_test_enc)[ex_g50k_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pipe_lgbm.named_steps[\"lgbmclassifier\"].predict(X_test_enc, raw_score=True)[ex_g50k_index] # raw model score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "shap.force_plot(\n",
    "    lgbm_explainer.expected_value[1],\n",
    "    test_lgbm_shap_values[1][ex_g50k_index, :],\n",
    "    X_test_enc.iloc[ex_g50k_index, :],\n",
    "    matplotlib=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The raw model score is bigger than the base value so the prediction is >50K (because we are consider the shap values for class 1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Observations: \n",
    "\n",
    "- Everything is with respect to class 1 here. \n",
    "- The base value for class 1 is -2.316. (You can think of this as the average raw score.) \n",
    "- We see the forces that drive the prediction. \n",
    "- That is, we can see the main factors pushing it from the base value (average over the dataset) to this particular prediction. \n",
    "- Features that push the prediction to a higher value are shown in red. \n",
    "- Features that push the prediction to a lower value are shown in blue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note: a nice thing about SHAP values is that the feature importances sum to the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lgbm_shap_values[1][ex_g50k_index, :].sum() + lgbm_explainer.expected_value[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Provides explainer for different kinds of models\n",
    "\n",
    "- [TreeExplainer](https://shap.readthedocs.io/en/latest/) (supports XGBoost, CatBoost, LightGBM) \n",
    "- [DeepExplainer](https://shap.readthedocs.io/en/latest/index.html#shap.DeepExplainer) (supports deep-learning models)\n",
    "- [KernelExplainer](https://shap.readthedocs.io/en/latest/index.html#shap.KernelExplainer) (supports kernel-based models)\n",
    "- [GradientExplainer](https://shap.readthedocs.io/en/latest/index.html#shap.GradientExplainer) (supports Keras and Tensorflow models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Can also be used to explain text classification and image classification \n",
    "- Example: In the picture below, red pixels represent positive SHAP values that increase the probability of the class, while blue pixels represent negative SHAP values the reduce the probability of the class. \n",
    "\n",
    "<img src=\"img/shap_image_explainer.png\" width=\"600\" height=\"600\">\n",
    "\n",
    "[Source](https://github.com/slundberg/shap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other tools\n",
    "\n",
    "- [lime](https://github.com/marcotcr/lime) is another package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're not already impressed, keep in mind:\n",
    "\n",
    "- So far we've only used sklearn models.\n",
    "- Most sklearn models have some built-in measure of feature importances.\n",
    "- On many tasks we need to move beyond sklearn, e.g. LightGBM, deep learning.\n",
    "- These tools work on other models as well, which makes them extremely useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Why do we want this information?\n",
    "\n",
    "Possible reasons:\n",
    "\n",
    "- Identify features that are not useful and maybe remove them.\n",
    "- Get guidance on what new data to collect.\n",
    "  - New features related to useful features -> better results.\n",
    "  - Don't bother collecting useless features -> save resources.\n",
    "- Help explain why the model is making certain predictions. \n",
    "  - Debugging, if the model is behaving strangely.\n",
    "  - Regulatory requirements.\n",
    "  - Fairness / bias.\n",
    "  - Keep in mind this can be used on **deployment** predictions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Questions for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True/False \n",
    "1. You train a random forest on a binary classification problem with two classes [neg, pos]. A value of 0.580 for feat1 given by `feature_importances_` attribute of your model means that increasing the value of feat1 will drive us towards positive class. \n",
    "2. eli5 can be used to get feature importances for non `sklearn` models. \n",
    "3. With SHAP you can only explain predictions on the training examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330]",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
